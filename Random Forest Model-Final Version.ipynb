{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af41bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "## some of them might not be used in this file\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c2910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on training set\n",
    "from sklearn.metrics import r2_score\n",
    "def R2(model, X_train,y_train):\n",
    "    \n",
    "    y_pred = model.predict(X_train)\n",
    "    SSE = np.sum((y_train - y_pred)**2)\n",
    "    SST = np.sum((y_train - np.mean(y_train))**2)\n",
    "                 \n",
    "    return (1 - SSE/SST)\n",
    "\n",
    "# Evaluation on test set\n",
    "def OSR2(model, X_test, y_test, y_train):\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    SSE = np.sum((y_test - y_pred)**2)\n",
    "    SST = np.sum((y_test - np.mean(y_train))**2)\n",
    "                 \n",
    "    return (1 - SSE/SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c27377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale columns\n",
    "def z_scale(df,y_name):\n",
    "    zscore=StandardScaler().fit(np.array(df[y_name]).reshape(-1, 1)) # 按原始训练集生成规则，即训练的均值和标准差\n",
    "    test=zscore.transform(np.array(df[y_name]).reshape(-1, 1))\n",
    "    df[y_name+'_scale']=test \n",
    "    df[y_name+'_scale']=round(df[y_name+'_scale'],5)#保留五位小数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09412b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "##one std rule\n",
    "\n",
    "def one_standard_error_rule(model, results, param_grid, n_splits):\n",
    "    \n",
    "    #assert neg_mean_squared_error == True # function is defined specifically for neg_mean_squared_error\n",
    "    \n",
    "    #find model with minimum error, then select the simplest model\n",
    "    #whose mean falls within 1 standard deviation of the minimum\n",
    "    \n",
    "    range_x = param_grid # results['param_'+list(param_grid.keys())[0]].data\n",
    "    std_vs_x  = pd.Series(results['std_test_score'], index = range_x)\n",
    "    sem_vs_x  = std_vs_x/np.sqrt(n_splits)\n",
    "    \n",
    "    mean_vs_x = pd.Series(results['mean_test_score'], index = range_x)        \n",
    "    mean_vs_x = mean_vs_x*(-1)\n",
    "    \n",
    "    x_min = mean_vs_x.idxmin()\n",
    "    sem = sem_vs_x[x_min]\n",
    "    \n",
    "    x_1se = mean_vs_x[mean_vs_x <= min(mean_vs_x) + sem].index.min()\n",
    "    \n",
    "\n",
    "    #if (model=='pcr'):\n",
    "        #x_1se = mean_vs_x[mean_vs_x <= min(mean_vs_x) + sem].index.min()\n",
    "    #elif (model=='ridge') | (model=='lasso'):\n",
    "        #x_1se = mean_vs_x[mean_vs_x <= min(mean_vs_x) + sem].index.max()\n",
    "        \n",
    "    #x_1se_idx = int(np.argwhere(range_x == x_1se)[0])\n",
    "    \n",
    "    return x_min, x_1se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaba139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Variance Inflation Factor for each explanatory variable\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def VIF(df, columns):\n",
    "    \n",
    "    values = sm.add_constant(df[columns]).values  # the dataframe passed to VIF must include the intercept term. We add it the same way we did before.\n",
    "    num_columns = len(columns)+1\n",
    "    vif = [variance_inflation_factor(values, i) for i in range(num_columns)]\n",
    "    \n",
    "    return pd.Series(vif[1:], index=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332ed70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "## some columns of the raw data won't be used in the final random forest model\n",
    "pgv=pd.read_csv('pgv_ca_county.csv')\n",
    "vul=pd.read_csv('vulnerability.csv')\n",
    "socio_econ=pd.read_excel('socio_econ.xlsx')\n",
    "resil=pd.read_excel('resilience.xlsx')\n",
    "district=pd.read_csv('county_district.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d6d893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shrink dimension on pgv data\n",
    "from sklearn.decomposition import PCA\n",
    "pgv_2=pgv.copy().drop(['lon','lat','county','state'],axis=1)\n",
    "pca = PCA(n_components=1)\n",
    "pgv['pgv_pca'] = pca.fit_transform(pgv_2) \n",
    "\n",
    "# scale pgv_pca\n",
    "z_scale(pgv,'pgv_pca')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0be585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize household income and total population in socio_econ\n",
    "\n",
    "z_scale(socio_econ,'house_inc')\n",
    "z_scale(socio_econ,'Total Population')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fed5ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill na with mean\n",
    "\n",
    "resil['recovery'] = resil['recovery'].fillna(resil['recovery'].mean())\n",
    "resil['resistance'] = resil['resistance'].fillna(resil['resistance'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c321b779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dataset\n",
    "raw_data=pd.merge(pgv,vul,left_on=['county'],right_on=['County'],how='left')\n",
    "raw_data=pd.merge(raw_data,socio_econ,left_on=['county'],right_on=['NAME'],how='left')\n",
    "raw_data=pd.merge(raw_data,resil,left_on=['county'],right_on=['County Name'],how='left')\n",
    "raw_data=pd.merge(raw_data,socio_econ_reduced,left_on=['county'],right_on=['NAME'],how='left')\n",
    "raw_data=pd.merge(raw_data,district,left_on=['county'],right_on=['County'],how='left')\n",
    "\n",
    "\n",
    "\n",
    "#raw['house_inc']=raw['house_inc'].astype('float')\n",
    "\n",
    "#earthquake risk index\n",
    "#raw_data['earthquake_risk']=raw_data.apply(lambda row:row['pgv_pca_scale']*row['vulnerability_index'],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e167d925",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b622d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b794832e",
   "metadata": {},
   "source": [
    "#### Final Random Forest Model: Using `pgv_pca` and `vulnerability index` seperately with `resistance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af677ace",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# useful subset of raw data\n",
    "df_ls=['lon', 'lat','county','District','pgv_pca_scale','vulnerability_index','pop_over65'\n",
    "       , 'disab_pct', 'edu_attain', 'unemployment',\n",
    "       'inc_inequal', 'health_insure_lack','mobile_home_pct',\n",
    "       'house_owner_pct', 'vacant_rent_pct', 'house_wo_vehi_pct', 'connect',\n",
    "       'hosp_cap', 'medi_prof_cap', 'school_cap', 'popul_change',\n",
    "       'hot_mot_cap', 'house_inc_scale', 'Total Population_scale','resistance']\n",
    "\n",
    "df_raw=raw_data[df_ls]\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b18b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for models\n",
    "df=df_raw.drop(['lon','lat'],axis=1)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9adbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset \n",
    "df_train, df_test = train_test_split(df, test_size=0.3, random_state=11)\n",
    "\n",
    "X_train=df_train.drop(['resistance'],axis=1)\n",
    "y_train=df_train['resistance']\n",
    "\n",
    "X_test=df_test.drop(['resistance'],axis=1)\n",
    "y_test=df_test['resistance']\n",
    "\n",
    "                     \n",
    "df_train.shape,df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29ef3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dtr=pd.get_dummies(X_train.drop(['District'],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c75ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train_dtr.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c065b13c",
   "metadata": {},
   "source": [
    "#### 2. Overall RandomForest Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709394bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data\n",
    "\n",
    "X_train_dtr=pd.get_dummies(X_train.drop(['District'],axis=1))\n",
    "X_test_dtr=pd.get_dummies(X_test.drop(['District'],axis=1))\n",
    "\n",
    "#CV on max features\n",
    "\n",
    "import time\n",
    "grid_values = {'max_features': np.linspace(5,77,15, dtype='int32'),\n",
    "              'n_estimators': [50],\n",
    "              'random_state': [1010]} \n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "rf = RandomForestRegressor() \n",
    "rf_cv = GridSearchCV(rf, param_grid=grid_values, scoring='r2', cv=5)\n",
    "rf_cv.fit(X_train_dtr, y_train)\n",
    "\n",
    "toc = time.time()\n",
    "print('time:', round(toc-tic, 2),'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1cfe05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## plotting\n",
    "\n",
    "max_features = rf_cv.cv_results_['param_max_features'].data\n",
    "R2_scores = rf_cv.cv_results_['mean_test_score']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.xlabel('max features', fontsize=16)\n",
    "plt.ylabel('CV R2', fontsize=16)\n",
    "plt.scatter(max_features, R2_scores, s=30)\n",
    "plt.plot(max_features, R2_scores, linewidth=3)\n",
    "plt.grid(True, which='both')\n",
    "plt.xlim([5, 76])\n",
    "plt.ylim([0.999, 1])\n",
    "plt.title('CV on Max Features',fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ca766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## one stand rule\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "n_components = rf_cv.cv_results_['param_max_features'].data\n",
    "R2_scores = rf_cv.cv_results_['mean_test_score']\n",
    "x_min, x_1se = one_standard_error_rule(model='rf_cv',\n",
    "                                       results=rf_cv.cv_results_,\n",
    "                                       param_grid=n_components,\n",
    "                                       n_splits=15)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.xlabel('max features', fontsize=16)\n",
    "plt.ylabel('CV R2', fontsize=16)\n",
    "plt.scatter(n_components, R2_scores, s=30)\n",
    "plt.axvline(x=x_min, color='m')\n",
    "plt.axvline(x=x_1se, color='c')\n",
    "plt.grid(True, which='both')\n",
    "plt.title('1std Rule',fontsize = 18)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404e2ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1f3f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data\n",
    "\n",
    "X_train_dtr=pd.get_dummies(X_train.drop(['District'],axis=1))\n",
    "X_test_dtr=pd.get_dummies(X_test.drop(['District'],axis=1))\n",
    "\n",
    "#Overall RandomForest Regression Model\n",
    "rf1 = RandomForestRegressor(n_estimators = 50, random_state=1010,max_features=56)\n",
    "rf1.fit(X_train_dtr, y_train)\n",
    "\n",
    "# Feature Importance\n",
    "\n",
    "feature_imp1 = pd.DataFrame({'Feature' : X_train_dtr.columns, \n",
    "              'Importance score': 100*rf1.feature_importances_}\n",
    "                           ).round(1).sort_values(by='Importance score',ascending=False)\n",
    "feature_imp1.head(40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fabcc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Random Forest R2:', r2_score(y_train,rf1.predict(X_train_dtr)))\n",
    "print('Random Forest OSR2:', round(OSR2(rf1, X_test_dtr, y_test, y_train), 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3552d44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46c48b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get important features for each district\n",
    "\n",
    "def rfr_district(df_train,df_test,district):\n",
    "    \n",
    "    # prepare dataset\n",
    "    df_train=df_train[df_train['District']==district]\n",
    "    df_test=df_test[df_test['District']==district]\n",
    "    \n",
    "    X_train=df_train.drop(['District','resistance'],axis=1)\n",
    "    y_train=df_train['resistance']\n",
    "    X_test=df_test.drop(['District','resistance'],axis=1)\n",
    "    y_test=df_test['resistance']\n",
    "    \n",
    "    # decision tree regressor\n",
    "    ## cross validation on `max_features` of the overall model above doesn't provide a outstanding parameter,\n",
    "    ## thus we decide to run the model with default value\n",
    "    rfr = RandomForestRegressor(min_samples_leaf=10, min_samples_split=15,n_estimators = 50, random_state=1010)\n",
    "    rfr.fit(X_train, y_train)\n",
    "    \n",
    "    # r2 and osr2\n",
    "    #dtr_r2=r2_score(y_train,dtr.predict(X_train))\n",
    "    r2=round(R2(rfr, X_train,y_train), 10)\n",
    "    osr2=round(OSR2(rfr, X_test, y_test, y_train), 10)\n",
    "    \n",
    "    # Feature Importance\n",
    "    feature_imp = pd.DataFrame({'Feature' : X_train.columns, \n",
    "              'Importance score': 100*rfr.feature_importances_}).round(10)\n",
    "    feature_imp = feature_imp.sort_values(by='Importance score',ascending=False).reset_index(drop = True)\n",
    "    \n",
    "    return r2,osr2,feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e31910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a table for top5 imprtant features of all districts\n",
    "dist_ls=['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12']\n",
    "Feature_1st_rfr=[]\n",
    "ImptScore_1st_rfr=[]\n",
    "Feature_2nd_rfr=[]\n",
    "ImptScore_2nd_rfr=[]\n",
    "Feature_3rd_rfr=[]\n",
    "ImptScore_3rd_rfr=[]\n",
    "Feature_4th_rfr=[]\n",
    "ImptScore_4th_rfr=[]\n",
    "Feature_5th_rfr=[]\n",
    "ImptScore_5th_rfr=[]\n",
    "\n",
    "for i in dist_ls:\n",
    "    impt_df_rfr = rfr_district(df_train,df_test,i)[2]\n",
    "    Feature_1st_rfr.append(impt_df_rfr['Feature'][0])\n",
    "    ImptScore_1st_rfr.append(impt_df_rfr['Importance score'][0])\n",
    "    Feature_2nd_rfr.append(impt_df_rfr['Feature'][1])\n",
    "    ImptScore_2nd_rfr.append(impt_df_rfr['Importance score'][1])\n",
    "    Feature_3rd_rfr.append(impt_df_rfr['Feature'][2])\n",
    "    ImptScore_3rd_rfr.append(impt_df_rfr['Importance score'][2])\n",
    "    Feature_4th_rfr.append(impt_df_rfr['Feature'][3])\n",
    "    ImptScore_4th_rfr.append(impt_df_rfr['Importance score'][3])\n",
    "    Feature_5th_rfr.append(impt_df_rfr['Feature'][4])\n",
    "    ImptScore_5th_rfr.append(impt_df_rfr['Importance score'][4])\n",
    "    \n",
    "df_ls= [dist_ls,\n",
    "        Feature_1st_rfr,ImptScore_1st_rfr,\n",
    "        Feature_2nd_rfr,ImptScore_2nd_rfr,\n",
    "        Feature_3rd_rfr,ImptScore_3rd_rfr, \n",
    "        Feature_4th_rfr,ImptScore_4th_rfr,\n",
    "        Feature_5th_rfr,ImptScore_5th_rfr]\n",
    "\n",
    "colname_ls=['District',\n",
    "            'Feature_1st','ImptScore_1st',\n",
    "            'Feature_2nd','ImptScore_2nd',\n",
    "            'Feature_3rd','ImptScore_3rd',\n",
    "            'Feature_4th','ImptScore_4th',\n",
    "            'Feature_5th','ImptScore_5th']\n",
    "\n",
    "rfr_result=pd.DataFrame(data=df_ls)\n",
    "\n",
    "rfr_result = pd.DataFrame(rfr_result.values.T,columns=colname_ls)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebe7b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eba3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_result.to_csv('rfr_result1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38b1274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

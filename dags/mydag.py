import datetime
import pendulum

import requests
from airflow.decorators import dag, task
from airflow.providers.postgres.hooks.postgres import PostgresHook

from utils.extractor import *

@dag(
    schedule_interval="0 0 * * *",
    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
    catchup=False,
    dagrun_timeout=datetime.timedelta(minutes=60),
)
def Etl():
    @task
    def get_data():
        # NOTE: configure this as appropriate for your airflow environment
        data_path = "/opt/airflow/dags/files/employees.csv"

        url = "https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/pipeline_example.csv"

        # response = extract_usgs_feed_all()

        # with open(data_path, "w") as file:
            # file.write(response.text)

        # postgres_hook = PostgresHook(postgres_conn_id="LOCAL")
        # conn = postgres_hook.get_conn()
        # cur = conn.cursor()
        # with open(data_path, "r") as file:
        #     cur.copy_expert(
        #         "COPY \"Employees_temp\" FROM STDIN WITH CSV HEADER DELIMITER AS ',' QUOTE '\"'",
        #         file,
        #     )
        # conn.commit()

    # @task
    # def merge_data():
    #     query = """
    #             DELETE FROM "Employees" e
    #             USING "Employees_temp" et
    #             WHERE e."Serial Number" = et."Serial Number";

    #             INSERT INTO "Employees"
    #             SELECT *
    #             FROM "Employees_temp";
    #             """
    #     try:
    #         postgres_hook = PostgresHook(postgres_conn_id="LOCAL")
    #         conn = postgres_hook.get_conn()
    #         cur = conn.cursor()
    #         cur.execute(query)
    #         conn.commit()
    #         return 0
    #     except Exception as e:
    #         return 1

    # get_data() >> merge_data()
    get_data()

dag = Etl()